---
title : Computing Discrete Fourier Transform
author : Nikhil Yewale
date: 29th April 2021
---

# Introduction

This is a general topic dealing with the commonly known ways of computing 1D Discrete Fourier Transform(DFT).
We try to explain the general $O(N^2)$ and $O(N.log(N))$ methods for computing DFT.
We also demonstrate the performance of our custom DFT functions with the julia bindings of the standard [FFTW](https://www.fftw.org/) package.

Let's start by importing the standard FFTW package.

```julia
using FFTW
```

Create a 1D signal or array of size $2^N$
```julia
x = rand(2^10);      ğ‘ = length(x);
```

Discrete Fourier transform(DFT) converts the sequence of $N$ complex numbers
${x_1 , x_2, .., x_N}$ into another sequence of $N$ complex numbers ${X_1 , X_2, .., X_N}$. This transformation is expressed as follows:
$$
\begin{align}
X_k = \sum_{n=1}^{N} x_n. e^{-\frac{2\pi i}{N}(k-1)(n-1)}
\end{align}
$$
where $k = 1,2,...,N$
```julia
# DFT_1 is plain definition of DFT... O(ğ‘Â²)
function DFT_1(x::AbstractArray; ğ‘::Int = length(x))
    X::Array{ComplexF64} = zeros(ComplexF64, ğ‘)
    for k âˆˆ 1:ğ‘
        for n âˆˆ 1:ğ‘
            X[k] += x[n]*â„¯^(-im*2Ï€*(k-1)*(n-1)/ğ‘)
        end
    end
    return X
end
DFT_1(x; ğ‘ = ğ‘) â‰ˆ fft(x)
```
`True !`, so our function does compute accurately.
Did you notice a little symmetry in the output of `DFT_1` function ? No ?
Take a look again !
```julia
DFT_1(rand(10))
```
The DFT values for indices `ğ‘:-1:(ğ‘/2 + 1 + 1)` are conjugate of the entries prior to index `ğ‘/2 + 1`.
This warrants some changes in the indices outer loop along with conditional evaluation.
`DFT_2` function employs these improvements as follows:
```julia
# DFT_2 with Little improvement over... O(ğ‘Â²)
function DFT_2(x::AbstractArray; ğ‘::Int = length(x))
    X::Array{ComplexF64} = zeros(ComplexF64, ğ‘)
    for k âˆˆ 1:(ğ‘Ã·2 + 1)
        for n âˆˆ 1:ğ‘
            X[k] += x[n]*â„¯^(-im*2Ï€*(k-1)*(n-1)/ğ‘)
        end
        if k â‰  1   # symmetric
            X[end - (k-2)] = conj(X[k])
        end
    end
    return X
end
DFT_2(x; ğ‘ = ğ‘) â‰ˆ fft(x)
```
`True !`,the improvements are accurately accomodated.

`DFT_1` and `DFT_2`, though accurate, their computing time scales as $O(N^2)$. Is there a better way ?

#### [Cooley-Tuckey algorithm](https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm) - The $O(N.log(N))$ way
This algorithm recursively breaks down a DFT of size $N$ into smaller DFTs.
The most commonly known recipe of Cooley-Tuckey algorithm is by recursively dividing the sequence of DFT into $N/2$ sizes at each step.This is called *radix 2 decimation-in-time(DIT)* case.
Naturally, this approach restricts the size of $N$ to be as power of $2$.

Radix-2 DIT computes the DFTs of the even-indexed inputs, and of the odd-indexed inputs, and then combines the results to produce the DFT of the whole sequence.
This step is performed recursively to scale-down the overall runtime to $O(N.log N)$. Detailed steps and pseudocode for *radix-2 DIT* can be found [here](https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm#The_radix-2_DIT_case).
`DIT_FFT_radix2` is our own implementation of *radix-2 DIT* case, and it too computes DFTs accurately!
```julia
# Cooley_Tuckey_FFT radix 2, based on divide and conquer
function DIT_FFT_radix2(x::AbstractArray; ğ‘::Int64 = length(x))
    Xâ‚ = Vector{ComplexF64}()
    Xâ‚‚ = Vector{ComplexF64}()
    if ğ‘ == 1
        return Array{ComplexF64}(x)
    else
        Xeven = DIT_FFT_radix2(x[1:2:end])  # recursion
        Xodd = DIT_FFT_radix2(x[2:2:end])
        for k âˆˆ 1:(ğ‘Ã·2)
            push!(Xâ‚, Xeven[k] + â„¯^(-2Ï€*im*(k-1)/ğ‘)*Xodd[k])
            push!(Xâ‚‚, Xeven[k] - â„¯^(-2Ï€*im*(k-1)/ğ‘)*Xodd[k])
        end
        return [Xâ‚; Xâ‚‚]
    end
end
DIT_FFT_radix2(x; ğ‘ = ğ‘) â‰ˆ fft(x)
```
Now, let's get greedy! Can we do even better(without parallelisation) ? There are multiple methods to optimise this even further, but it always scales as $O(N.log(N))$.
We can still certainly speed up these $O(N.log(N))$ operations by making the recursion more efficient.
One way is **memoization**, where we save some expensive function calls, so that it can be reused when the same inputs occur again later.

#### [Memoization](https://en.wikipedia.org/wiki/Memoization)
A really good example to understand memoization is by studying a [memoised factorial function](https://en.wikipedia.org/wiki/Memoization#Overview).
Let us make the function remember some of it's old calls and reuse this past knowledge for speedy future unknown calls.

Lucikly(lazily), Julia package `Memoize` provides a macro `@memoize` to *memoize* the function `DIT_FFT_radix2_mem`. !
And this too accurately returns the required DFT.
```julia
# Memoization of Cooley_Tuckey_FFT radix 2, based on divide and conquer
using Memoize
@memoize function DIT_FFT_radix2_mem(x::AbstractArray; ğ‘::Int64 = length(x))
    Xâ‚ = Vector{ComplexF64}()
    Xâ‚‚ = Vector{ComplexF64}()
    if ğ‘ == 1
        return Array{ComplexF64}(x)
    else
        Xeven = DIT_FFT_radix2(x[1:2:end])  # recursion
        Xodd = DIT_FFT_radix2(x[2:2:end])
        for k âˆˆ 1:(ğ‘Ã·2)
            push!(Xâ‚, Xeven[k] + â„¯^(-2Ï€*im*(k-1)/ğ‘)*Xodd[k])
            push!(Xâ‚‚, Xeven[k] - â„¯^(-2Ï€*im*(k-1)/ğ‘)*Xodd[k])
        end
        return [Xâ‚; Xâ‚‚]
    end
end
DIT_FFT_radix2_mem(x; ğ‘ = ğ‘) â‰ˆ fft(x)
```
Time for some measurements now!
```julia
using BenchmarkTools
@btime DFT_1(x);
```
`DFT_1` Not so good ! `ğŸ˜¢`

```julia
@btime DFT_2(x);
```
`DFT_2` Little better than  `DFT_1`! `ğŸ˜…`

```julia
@btime DIT_FFT_radix2(x);
```
`DIT_FFT_radix2`, Wow ! `ğŸ˜`

```julia
@btime DIT_FFT_radix2_mem(x);
```
`DIT_FFT_radix2_men`, I love memoization ğŸ’“

`fft` and `plan_fft` are standard functions in `FFTW` julia bindings. `plan_fft` is little better than `fft`, but both of them lag behind the memoized function !
To know why, I will update the page after I understand the inner-workings of `FFTW` package.
```julia
@btime fft(x);      # less efficient built-in
```
```julia
@btime plan_fft(x); # more efficient built-in
```
Hope you had fun reading about DFT !
#### Note
- This page is open for updation on more content related to computing DFT, and you are welcome to open PR/issues !
- The functions are meant for computing 1D DFT at the moment. Generalizations for higher-dimensions will follow soon.
- The functions are not yet written to be executed on multiple-threads or do not support any sort of parallelism. That too shall be a topic for another day.
- The function `DIT_FFT_radix2` and `DIT_FFT_radix2_mem` compute DFTs only for arrays of size $2^N$
